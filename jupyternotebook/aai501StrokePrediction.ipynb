{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0011bfd8",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ccd36ba-41b3-4be0-84e9-bbfa10b07f9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SVC\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SimpleImputer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder, MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2, RFE\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "# Logistic Regresss is not yet supported by SHAP\n",
    "import shap\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd66282-2c26-437d-99e9-b681ea846dc4",
   "metadata": {},
   "source": [
    "## Load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a56502-cf3d-4fab-acf0-2f417e7786f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dataset/strokeDataSet.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f53bfe",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d18be41-e62d-4263-a675-0fddd9765b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69b1654-067f-4c6b-937b-6294653351af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af4572d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b527c47",
   "metadata": {},
   "source": [
    "### age:\n",
    "    Mean age is 43.2 years, suggesting the dataset includes both younger and older adults. \\\n",
    "    Std Dev of 22.6 is large, age is widely spread. \\\n",
    "    Min value is 0.08: possibly an infant. \\\n",
    "    Max is 82: includes elderly individuals, a known stroke risk group.\n",
    "\n",
    "\n",
    "### avg_glucose_level:\n",
    "    Mean glucose is 106.15 mg/dL. \\\n",
    "    Std Dev is 45.28: indicates significant variation. \\\n",
    "    Max of 271.74 is very high: suggests presence of diabetic/hyperglycemic individuals. \\\n",
    "\n",
    "High glucose levels are risk factors for stroke, this feature may be important in modeling.\n",
    "\n",
    "### bmi (Body Mass Index)\n",
    "    Mean is 28.89: close to the overweight range (25â€“29.9). \\\n",
    "    Max is 97.6: likely indicates extreme obesity. \\\n",
    "    Std Dev is 7.85: some individuals may be underweight or obese.\n",
    "\n",
    "BMI is a known cardiovascular risk factor, useful for stroke prediction.\n",
    "\n",
    "### stroke (target variable)\n",
    "    Mean of 0.049 = 4.9% of patients had a stroke.\n",
    "    Values are only 0 or 1: this is a binary classification task.\n",
    "    Highly imbalanced: 95% non-stroke, 5% stroke.\n",
    "\n",
    "Class imbalance is a challenge, models will need techniques like SMOTE or class weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24aafdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07bd348",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()\n",
    "df = df[df['gender'] != 'Other'] # there is only 1 Other gender, so we can drop it\n",
    "df.dropna(subset=['bmi'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee182ca0",
   "metadata": {},
   "source": [
    "### Data Overview & Cleaning\n",
    "This dataset contains a total of 5,110 rows with no duplicate, and the 'bmi' parameter has 201 missing values. These missing values were filled using the median, based on the assumption that the individuals with missing 'bmi' entries have typical or average bmi values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632d866a",
   "metadata": {},
   "source": [
    "### Columns:\n",
    "1) id: unique identifier\n",
    "2) gender: \"Male\", \"Female\"\n",
    "3) age: age of the patient\n",
    "4) hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n",
    "5) heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n",
    "6) ever_married: \"No\" or \"Yes\"\n",
    "7) work_type: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n",
    "8) Residence_type: \"Rural\" or \"Urban\"\n",
    "9) avg_glucose_level: average glucose level in blood\n",
    "10) bmi: body mass index\n",
    "11) smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n",
    "12) stroke: 1 if the patient had a stroke or 0 if not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c135ecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = ['age', 'avg_glucose_level', 'bmi']\n",
    "\n",
    "for col in numerical_features:\n",
    "    sns.boxplot(x=df[col])\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "    plt.savefig(f'boxplot_{col}.png')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94df9225",
   "metadata": {},
   "source": [
    "### Interpret boxplots\n",
    "\n",
    "Boxplots show outliers in bmi and glucose level, but this is reflect the real patient conditions. A stroke patient may actually have a BMI of 90 or glucose of 270, so removing these would hide critical clinical cases!\n",
    "\n",
    "They are rare but valid, outliers in this dataset can help catch severe cases that models should learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44aa0052",
   "metadata": {},
   "source": [
    "# Ultilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a95f1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']\n",
    "for col in categorical_cols:\n",
    "    df[col] = df[col].astype('category')\n",
    "\n",
    "# encoding\n",
    "df_encoded = pd.get_dummies(df.drop(columns=['id']))\n",
    "\n",
    "print(df_encoded.columns.tolist())\n",
    "\n",
    "selected_features = [\n",
    "    \"age\",\n",
    "    \"avg_glucose_level\",\n",
    "    \"hypertension\",\n",
    "    \"heart_disease\",\n",
    "    \"smoking_status_Unknown\",\n",
    "    \"ever_married_Yes\",\n",
    "    \"work_type_Self-employed\",\n",
    "    \"work_type_children\",\n",
    "    \"work_type_Govt_job\"\n",
    "]\n",
    "\n",
    "# Split features and labels\n",
    "X = df_encoded[selected_features]\n",
    "y = df_encoded['stroke']\n",
    "# Split data into training (60%), validation (20%), and test (20%) sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val) \n",
    "\n",
    "# Scale the features\n",
    "#scaler = StandardScaler()\n",
    "X_train_scaled = StandardScaler().fit_transform(X_train)\n",
    "X_val_scaled = StandardScaler().fit_transform(X_val)\n",
    "X_test_scaled = StandardScaler().fit_transform(X_test)\n",
    "\n",
    "# Apply SMOTE to handle class imbalance\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2f6dea",
   "metadata": {},
   "source": [
    "### Formal Feature Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6acf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FormalFeatureSelection:\n",
    "    def __init__(self, df, features, label):\n",
    "        self.df = df\n",
    "        self.features = features\n",
    "        self.label = label\n",
    "        \n",
    "    def selectTopFeatures(self, top_n_anova=5, top_n_chi=5, top_n_rf=5, include_ref=5):\n",
    "        self.featureSelectionSummary()\n",
    "        top_features = self.feature_selection_summary\n",
    "        anova_top = (\n",
    "            top_features[\"Anova F\"].sort_values(ascending=False).head(top_n_anova).index\n",
    "        )\n",
    "        chi2_top = (\n",
    "            top_features[\"CHI2\"].sort_values(ascending=False).head(top_n_chi).index\n",
    "        )\n",
    "        rf_top = (\n",
    "            top_features[\"RF Importance\"].sort_values(ascending=False).head(top_n_rf).index\n",
    "        )\n",
    "        rfe_top = (\n",
    "            top_features[\"RFE Support\"].sort_values(ascending=False).head(include_ref).index\n",
    "        )\n",
    "        self.top_features = set(anova_top).union(chi2_top).union(rf_top).union(rfe_top)\n",
    "        display(top_features.loc[list(self.top_features)])\n",
    "\n",
    "    def featureSelectionSummary(self):\n",
    "        features = self.df.drop(columns=self.label)\n",
    "        self.labels = self.df[self.label]\n",
    "        self.features = pd.get_dummies(features)        \n",
    "        \n",
    "        #Anova F-Test\n",
    "        skb_f = SelectKBest(score_func=f_classif, k=\"all\").fit(self.features, self.labels)\n",
    "        anova_scores = pd.Series(skb_f.scores_, index=self.features.columns).sort_values(ascending=False)\n",
    "\n",
    "        #Normalize for Chi2\n",
    "        scaler = MinMaxScaler()\n",
    "        features_scaled = pd.DataFrame(\n",
    "            scaler.fit_transform(self.features), columns=self.features.columns\n",
    "        )\n",
    "        \n",
    "        #Chi2\n",
    "        skb_chi2 = SelectKBest(score_func=chi2, k=\"all\").fit(features_scaled, self.labels)\n",
    "        chi2_scores = pd.Series(skb_chi2.scores_, index=self.features.columns).sort_values(ascending=False)\n",
    "\n",
    "        #RandomForestImportances\n",
    "        rf = RandomForestClassifier(random_state=0)\n",
    "        rf.fit(self.features, self.labels)\n",
    "        rf_scores = pd.Series(\n",
    "            rf.feature_importances_, index=self.features.columns\n",
    "        ).sort_values(ascending=False)\n",
    "\n",
    "        #RFE - Logistic Regression\n",
    "        lr = LogisticRegression(max_iter=4000)\n",
    "        rfe = RFE(estimator=lr, n_features_to_select=10)\n",
    "        rfe.fit(self.features, self.labels)\n",
    "        rfe_support = pd.Series(rfe.support_, index=self.features.columns)\n",
    "\n",
    "        \n",
    "        self.feature_selection_summary = pd.DataFrame(\n",
    "            {\n",
    "                \"Anova F\": anova_scores,\n",
    "                \"CHI2\": chi2_scores,\n",
    "                \"RF Importance\": rf_scores,\n",
    "                \"RFE Support\": rfe_support.astype(bool)\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1878632b",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d7d2f4",
   "metadata": {},
   "source": [
    "## Univariate Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756d4630",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['age', 'avg_glucose_level', 'bmi']\n",
    "df[num_cols].hist(bins=20, figsize=(12, 8))\n",
    "plt.suptitle('Histograms of Numerical Features')\n",
    "plt.savefig('histograms_numerical_features.png')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5475c038",
   "metadata": {},
   "source": [
    "Numerical histograms: \"age, avg_glucose_level, bmi\" show right-skewed distributions, especially \"avg_glucose_level\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbdfbb5",
   "metadata": {},
   "source": [
    "Stroke distribution shows heavy imbalance between class 1 (stroke) and class 0 (no stroke) in the dataset. Class 1 is much less than Class 0. This could lead to poor model evaluation. To address this issue, we will use stratified sampling, SMOTE, and class weighting to improve the prediction of class 1 (stroke).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445e8d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_cols:\n",
    "    \n",
    "    sns.countplot(x=col, data=df)\n",
    "    plt.title(f'{col} distribution')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.savefig(f'countplot_{col}.png')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6272528c",
   "metadata": {},
   "source": [
    "Categorical countplots like gender, smoking_status, work_type show: most patients are female and have never smoked. Majority work in the private sector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f4ffc5",
   "metadata": {},
   "source": [
    "# Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd14987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_cols:\n",
    "    pd.crosstab(df[col], df['stroke'], normalize='index').plot(kind='bar', stacked=True)\n",
    "    plt.title(f'Stroke by {col}')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.savefig(f'stroke_by_{col}.png')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f54afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in num_cols:\n",
    "    sns.boxplot(x='stroke', y=col, data=df)\n",
    "    plt.title(f'{col} by Stroke')\n",
    "    plt.savefig(f'boxplot_{col}_by_stroke.png') \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418bb5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df[['age', 'hypertension', 'heart_disease', 'avg_glucose_level', 'bmi', 'stroke']].corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.savefig('correlation_matrix.png')\n",
    "plt.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce39be7",
   "metadata": {},
   "source": [
    "### Summary of Bivariate Analysis\n",
    "| Feature        | Strength of Association with Stroke  | Notes                                                     |\n",
    "| -------------- | -----------------------------------  | ------------------------------------------------------    |\n",
    "| Age            | Strong                               | Older people much more likely to have stroke              |\n",
    "| Glucose Level  | Moderate                             | High glucose is a risk factor                             |\n",
    "| Smoking Status | Moderate                             | \"Formerly smoked\" and \"smokes\" show elevated stroke risk  |\n",
    "| Hypertension   | Moderate                             | Medically important though correlation is weak            |\n",
    "| Heart Disease  | Moderate                             | Similar to hypertension                                   |\n",
    "| Ever Married   | Weak to Moderate                     | Marginal relationship with stroke\n",
    "| Work Type      | Weak to Moderate                     | \"Self-employed\" and \"Govt job\" higher                     |\n",
    "| BMI            | Weak                                 | Not strongly correlated alone                             |\n",
    "| Gender         | Very Weak                            | No major difference                                       |\n",
    "| Residence Type | None                                 | Urban and rural similar                                   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8beb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = FormalFeatureSelection(df.drop(columns=['id']), df.columns.drop('stroke').tolist(), 'stroke')\n",
    "fs.selectTopFeatures(top_n_anova=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3902f23a",
   "metadata": {},
   "source": [
    "The following features were chosen based on a combination of statistical tests (ANOVA F-test, ChiÂ²), model-based importance (Random Forest), and wrapper methods (RFE with Logistic Regression). These features consistently demonstrated strong association with the likelihood of stroke or were retained by multiple selection methods: \\\n",
    "            [\"age\",\"avg_glucose_level\", \\\n",
    "            \"hypertension\",\"heart_disease\", \\\n",
    "            \"smoking_status_Unknown\", \\\n",
    "            \"ever_married_Yes\", \\\n",
    "            \"work_type_Selfemployed\", \\\n",
    "            \"work_type_children\", \\\n",
    "            \"work_type_Govt_job\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72dce08",
   "metadata": {},
   "source": [
    "# ANN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546c3aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ANN model\n",
    "ann_model = MLPClassifier(hidden_layer_sizes=(64, 32), activation='relu', max_iter=500, random_state=42)\n",
    "ann_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Evaluate ANN on validation and test sets\n",
    "val_preds_ann = ann_model.predict(X_val_scaled)\n",
    "test_preds_ann = ann_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"ANN Validation Accuracy:\", accuracy_score(y_val, val_preds_ann))\n",
    "print(\"ANN Test Accuracy:\", accuracy_score(y_test, test_preds_ann))\n",
    "print(\"ANN Validation Classification Report:\\n\", classification_report(y_val, val_preds_ann))\n",
    "print(\"ANN Test Classification Report:\\n\", classification_report(y_test, test_preds_ann))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3418cfc1",
   "metadata": {},
   "source": [
    "# Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99728b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression model\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Evaluate Logistic Regression\n",
    "val_preds_lr = lr_model.predict(X_val_scaled)\n",
    "test_preds_lr = lr_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"Logistic Regression Validation Accuracy:\", accuracy_score(y_val, val_preds_lr))\n",
    "print(\"Logistic Regression Test Accuracy:\", accuracy_score(y_test, test_preds_lr))\n",
    "print(\"Logistic Regression Validation Classification Report:\\n\", classification_report(y_val, val_preds_lr, zero_division=0))  # set undefined precision to zero\n",
    "print(\"Logistic Regression Test Classification Report:\\n\", classification_report(y_test, test_preds_lr, zero_division=0))  # set undefined precision to zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc03d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': lr_model.coef_[0]\n",
    "}).sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "print(\"Logistic Regression Coefficients:\\n\", coef_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1f505c",
   "metadata": {},
   "source": [
    "In this dataset, age has the strongest positive contributor to stroke risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45e0916",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491f20aa",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8322f4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost\n",
    "xgb = XGBClassifier(\n",
    "    use_label_encoder=False, # use_label_encoder=False to avoid deprecared automic encoding for target values\n",
    "    eval_metric='logloss', # eval_metric='logloss' to optimize for binary classification\n",
    "    random_state=42,\n",
    "    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum(), # handle class imbalance\n",
    "    max_depth=4, # control tree complexity\n",
    "    learning_rate=0.1, # smaller learning steps\n",
    "    n_estimators=100, # number of boosting rounds\n",
    "    subsample=0.8, # random sample of rows\n",
    "    colsample_bytree=0.8 # random sample of features\n",
    ")\n",
    "xgb.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate XGBoost\n",
    "val_preds_xgb = xgb.predict(X_val_scaled)\n",
    "test_preds_xgb = xgb.predict(X_test_scaled)\n",
    "\n",
    "y_probs = xgb.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "\n",
    "print(\"XGBoost Validation Accuracy:\", accuracy_score(y_val, val_preds_xgb))\n",
    "print(\"XGBoost Test Accuracy:\", accuracy_score(y_test, test_preds_xgb))\n",
    "print(\"XGBoost Validation Classification Report:\\n\", classification_report(y_val, val_preds_xgb))\n",
    "print(\"XGBoost Test Classification Report:\\n\", classification_report(y_test, test_preds_xgb)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46103043",
   "metadata": {},
   "source": [
    "### Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c70fb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lgb_model = lgb.LGBMClassifier(random_state=42)\n",
    "lgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "#Evaluate Light GBM\n",
    "val_preds_lgbm = lgb_model.predict(X_val_scaled)\n",
    "test_preds_lgbm = lgb_model.predict(X_test_scaled)\n",
    "\n",
    "y_probs = lgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# lgb_auc = roc_auc_score(y_test, lgb_model.predict_proba(X_test)[:,1])\n",
    "\n",
    "print(\"LGBM Validation Accuracy:\", accuracy_score(y_val, val_preds_lgbm))\n",
    "print(\"LGBM Test Accuracy:\", accuracy_score(y_test, test_preds_lgbm))\n",
    "print(\"LGBM Validation Classification Report:\\n\", classification_report(y_val, val_preds_lgbm))\n",
    "print(\"LGBM Test Classification Report:\\n\", classification_report(y_test, test_preds_lgbm)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b135212",
   "metadata": {},
   "source": [
    "### Cat Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9d8642",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = df.copy(deep=True).drop(columns=['id'])\n",
    "\n",
    "#Fill missing values\n",
    "# Separate numeric and categorical columns\n",
    "numeric_cols = df_cat.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Impute numeric columns with median\n",
    "df_cat[numeric_cols] = num_imputer.fit_transform(df_cat[numeric_cols])\n",
    "\n",
    "X_cat = df_cat.drop('stroke', axis=1)\n",
    "y_cat = df_cat['stroke']\n",
    "\n",
    "X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(\n",
    "    X_cat, y_cat, stratify=y_cat, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 7. Get indices of categorical columns (for CatBoost)\n",
    "cat_features_indices = [X_cat.columns.get_loc(col) for col in categorical_cols]\n",
    "\n",
    "cat_model = CatBoostClassifier(verbose=0, random_state=42)\n",
    "cat_model.fit(X_train_cat, y_train_cat, cat_features=cat_features_indices)\n",
    "\n",
    "test_preds_cat = cat_model.predict(X_test_cat)\n",
    "\n",
    "y_probs_cat = cat_model.predict_proba(X_test_cat)[:, 1]\n",
    "\n",
    "\n",
    "print(\"Cat Test Accuracy:\", accuracy_score(y_test, test_preds_cat))\n",
    "print(\"Cat Test Classification Report:\\n\", classification_report(y_test, test_preds_cat)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d086f5",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5659731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM model\n",
    "svm_model = SVC(kernel='linear', random_state=42)  # Using linear kernel for SVM\n",
    "svm_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Evaluate SVM Regression\n",
    "val_preds_svm = svm_model.predict(X_val_scaled)\n",
    "test_preds_svm = svm_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"SVM Validation Accuracy:\", accuracy_score(y_val, val_preds_svm))\n",
    "print(\"SVM Test Accuracy:\", accuracy_score(y_test, test_preds_svm))\n",
    "print(\"SVM Validation Classification Report:\\n\", classification_report(y_val, val_preds_svm, zero_division=0)) # set undefined precision to zero    \n",
    "print(\"SVM Test Classification Report:\\n\", classification_report(y_test, test_preds_svm, zero_division=0)) # set undefined precision to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee3c7a3",
   "metadata": {},
   "source": [
    "# Compute and plot model prediction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc590dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary with model names and their predictions\n",
    "models_predictions = {\n",
    "    \"ANN Classifier\": test_preds_ann,\n",
    "    \"Logistic Regression\": test_preds_lr,\n",
    "    \"XGBoost\": test_preds_xgb,\n",
    "    \"SVM\": test_preds_svm\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acf2c69",
   "metadata": {},
   "source": [
    "# Caculate and plot models confusion matrix and heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b36fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each model and compute confusion matrix\n",
    "for model_name, predictions in models_predictions.items():\n",
    "    conf_matrix = confusion_matrix(y_test, predictions)\n",
    "    print(f\"{model_name} Confusion Matrix:\\n{conf_matrix}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aac84bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each model and plot confusion matrix\n",
    "plt_num = 1\n",
    "plt.figure(figsize=(16,11)) # Set figure size\n",
    "for model_name, predictions in models_predictions.items():\n",
    "    conf_matrix = confusion_matrix(y_test, predictions)\n",
    "    plt.subplot(2, 2, plt_num)\n",
    "    sns.heatmap(conf_matrix, cmap=\"BuGn\", annot=True, cbar_kws={\"label\":\"Color Bar\"}) \n",
    "    plt.suptitle('Models Confusion Matrix', fontsize=15, y=1)  \n",
    "    plt.title(model_name), plt.xlabel('Prdicted'), plt.ylabel('Actual')\n",
    "    plt.savefig('Confusion_Matrix.png')\n",
    "    plt_num +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1783791",
   "metadata": {},
   "source": [
    "Use SHAP (SHapley Additive exPlanations) tool for interpretability machine learning model. SHAP is a framework that explains the output of machine learning models using Shapley values from game theory. It fairly distributes each feature's contribution to individual predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23abc81",
   "metadata": {},
   "source": [
    "Reference: \n",
    "Lundberg, S. M. (n.d.). Basic SHAP interaction value example in XGBoost. SHAP Documentation. https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/tree_based_models/Basic%20SHAP%20Interaction%20Value%20Example%20in%20XGBoost.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405ea174",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model_name = [xgb]  # Use model objects, not strings\n",
    "explainer = shap.TreeExplainer(xgb)\n",
    "shap_values = explainer.shap_values(X_test_scaled)\n",
    "shap.summary_plot(shap_values, X_test_scaled, feature_names=X.columns, show=False)\n",
    "plt.savefig('SHAP_values.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (assignment)",
   "language": "python",
   "name": "assignment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
